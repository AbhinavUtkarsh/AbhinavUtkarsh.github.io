[ {"title":"Emotion-Driven Editing of GaussianAvatars","team":"Abhinav Utkarsh","institute":"Technical University of Munich, Germany","instituteDE":"Technische Universität München, DE","description":"Introduced EMO-GA: a novel pipeline that uses 2-D diffusion edits to supervise multi-view 3-D FLAME tracking and tile-based differentiable rendering, inducing previously unseen emotions in neutral 3-D head avatars and achieving state-of-the-art emotion depiction for GaussianAvatars—complete with wrinkles, nuanced shading and subtle skin-tone shifts. Compared with vanilla GaussianAvatars, EMO-GA cuts L1 error by ~7 %, raises PSNR by ≈ 0.5 dB and boosts SSIM to 0.949 for higher visual fidelity. Identity preservation also outperforms 2-D diffusion baselines, maintaining mean ArcFace distance ≤ 0.32 and VGG-Face distance ≤ 0.27 across subjects. The model renders view-consistent, high-fidelity novel poses in real time and shows that noisy 2-D generative supervision can replace large labelled 3-D emotion datasets.","descriptionDE":"EMO-GA ist eine neuartige Pipeline, die 2-D-Diffusionsedits nutzt, um Multi-View-FLAME-Tracking und tile-basiertes differentielles Rendering zu supervisieren. Dadurch erhalten neutrale 3-D-Kopf-Avatare bislang ungesehene Emotionen und eine state-of-the-art-Darstellung von Gefühlsausdrücken in GaussianAvatars – inklusive Falten, differenzierter Schattierung und subtiler Hautfarbnuancen. Gegenüber reinen GaussianAvatars reduziert EMO-GA den L1-Fehler um ca. 7 %, erhöht den PSNR um ≈ 0,5 dB und steigert den SSIM-Wert auf 0,949. Zugleich übertrifft das Verfahren 2-D-Diffusionsbaselines bei der Identitätserhaltung: mittlere ArcFace-Distanz ≤ 0,32 und VGG-Face-Distanz ≤ 0,27. Das Modell rendert konsistente Neuansichten in Echtzeit und zeigt, dass rauschhafte 2-D-Generierung große, annotierte 3-D-Emotionsdatensätze ersetzen kann.","keywords":["3D Computer Vision","Gaussian Splatting","Diffusion Models","Differentiable Rendering"],"image": "./images/EMOGA.png","url":"https://abhinavutkarsh.com/Emotion-Driven-Editing-of-Gaussian-Avatars/"},
  { "title": "Multimodal Features for 3D Point Cloud Anomaly Detection", "team" : "Abhinav Utkarsh, Matthias Pouleau , Malaz Tamim, Sayak Dutta" ,"institute": "Technical University of Munich, Germany", "instituteDE": "Technische Universität München, DE", "description": "Achieved up to 2% performance enhancement on Image AU-ROC and Pixel-AU-PRO metrics using recently (mid-2023) released Real-AD-3D dataset for anomaly detection. Implemented PointNet and PointNet++ for 3D feature extraction in a conventional descriptor-based pipeline, while integrating noising and patching techniques for local geometries. As the processed pointcouds were high-resolution, we deployed an SDF Network for a high-resolution 3D feature extraction, enhancing spatial comprehension in the feature space","descriptionDE" :"", "keywords": ["3D Computer Vision", "Deep learning"], "image": "", "url":"https://www.github.com/AbhinavUtkarsh/3D-Anomaly-Detection"},
  { "title": "Large Language Models for Structuring Radiology Reports", "team" : "Abhinav Utkarsh, Zeineb Ben Chaaben , Baris Sozudogru, Feng Dang" ,"institute": "Technical University of Munich, Germany", "instituteDE": "Technische Universität München, DE", "description": "Improved Level 1 data generation F1 score by 26% for a synthetic structured radiology report generation task, by designing a Question Answering Pipeline with handcrafted leaky prompts, essentially guiding the model to generate higher-quality structured radiology reports. At the same time, we achieved an 15x enhancement in Level 3 generations by fine-tuning (LoRA) an open source LLM (Vicuna 13B) on existing annotated ground truth. Improved Level 3 Recall by 35% from baseline with an image based visual question answering deep learning-based model Rad-Restruct by pretraining on a synthetically generated large-scale MIMIC dataset and training on annotated ground truth","descriptionDE" :"", "keywords": ["Large Language Models (LLM)", "Vicuna", "LoRA", "Rad-Restruct"], "image": "", "url":"https://collab.dvb.bayern/display/TUMmlmi/MLMI+Summer+2023?preview=/69050519/69050563/MLMI_Proposal_Report_Structuring.pdf"},
  { "title": "Autonomous Drones with ROS", "institute": "Technical University of Munich, Germany", "team" : "Milena Eisemann, Samyak Jain, Abhinav Utkarsh, Fabienne Greier, Christian Mändle" ,"instituteDE": "Technische Universität München, DE", "description": "Executed the autonomous coordination of two drones for 3D mapping within a 50×50 m² unity-based environment with ROS 1. Collaborated in a team of five, developing the navigation, perception and state machine packages while refining the PID controller gain values","descriptionDE" :"" ,"keywords": ["ROS1", "Unity", "Linux"], "image": "./images/ROS_2.png", "url":"https://www.youtube.com/watch?v=I9YYYC3NxW4&feature=youtu.be" },
  { "title": "Seminar: Dense Depth Priors for Neural Radiance Fields from Sparse Input Views (Barbara Roessle et al.)", "institute": "Technical University of Munich, Germany", "team" : "Abhinav Utkarsh under supervision of Patrick Ruhkamp" ,"instituteDE": "Technische Universität München, DE", "description": "Conducted a seminar showcasing the significance of novel views, volume rendering, and positional encoding techniques, where I elaborated on vanilla Neural Radiance Fields (NeRF) and depth-supervised enhancements (DS-NeRF). Finally, I concluded by focusing on applying dense depth priors to NeRF trained on sparse views.","descriptionDE" :"" ,"keywords": ["NeRF", "Seminar"], "image": "", "url":"https://drive.google.com/file/d/17fPQCcu149Ar3b6lVGjVzjRflx87vzkt/view?usp=drive_link" }
]
